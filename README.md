# Social Media Analytics Pipeline

A robust data engineering pipeline that fetches, transforms, and analyzes social media data from multiple platforms, computing engagement metrics and insights.

## ğŸ¯ Project Overview

This project implements a comprehensive social media analytics pipeline that:

- **Collects data** from Twitter, Facebook, and YouTube APIs
- **Normalizes data** from multiple platforms into a unified schema
- **Computes analytics** including daily engagement metrics, top posts, and moving averages
- **Automates execution** using Apache Airflow for daily runs
- **Stores results** in multiple formats (JSON, CSV, SQLite/PostgreSQL)

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Data Sources  â”‚    â”‚   Data Storage  â”‚    â”‚   Analytics     â”‚
â”‚                 â”‚    â”‚                 â”‚    â”‚                 â”‚
â”‚ â€¢ Twitter API   â”‚â”€â”€â”€â–¶â”‚ â€¢ SQLite/PostgreSQL â”‚â”€â”€â”€â–¶â”‚ â€¢ Daily Metrics â”‚
â”‚ â€¢ Facebook API  â”‚    â”‚ â€¢ JSON Files    â”‚    â”‚ â€¢ Top Posts     â”‚
â”‚ â€¢ YouTube API   â”‚    â”‚ â€¢ CSV Files     â”‚    â”‚ â€¢ Moving Averagesâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                       â”‚                       â”‚
         â–¼                       â–¼                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Collectors    â”‚    â”‚   Processors    â”‚    â”‚   Airflow DAG   â”‚
â”‚                 â”‚    â”‚                 â”‚    â”‚                 â”‚
â”‚ â€¢ BaseCollector â”‚    â”‚ â€¢ DataProcessor â”‚    â”‚ â€¢ Daily Scheduleâ”‚
â”‚ â€¢ Twitter       â”‚    â”‚ â€¢ Analytics     â”‚    â”‚ â€¢ Error Handlingâ”‚
â”‚ â€¢ Facebook      â”‚    â”‚ â€¢ Validation    â”‚    â”‚ â€¢ Notifications â”‚
â”‚ â€¢ YouTube       â”‚    â”‚ â€¢ Insights      â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ Features

### Data Collection
- **Multi-platform support**: Twitter, Facebook, YouTube
- **Flexible queries**: Search by keywords, hashtags, or user accounts
- **Rate limiting**: Respectful API usage with retry mechanisms
- **Error handling**: Robust error handling and logging

### Data Processing
- **Unified schema**: Normalized data model across all platforms
- **Engagement metrics**: Likes, comments, shares, and calculated engagement scores
- **Data validation**: Pydantic models for data integrity
- **Missing data handling**: Graceful handling of incomplete data

### Analytics
- **Daily metrics**: Per-platform engagement statistics
- **Top posts**: Overall and platform-specific top performers
- **Moving averages**: 7-day and 30-day engagement trends
- **Platform comparison**: Cross-platform performance analysis
- **Anomaly detection**: Statistical outlier identification

### Automation
- **Airflow DAG**: Daily scheduled execution
- **Quality checks**: Data validation and pipeline monitoring
- **Notifications**: Success/failure alerts
- **Error recovery**: Automatic retry mechanisms

## ğŸ“‹ Requirements

### System Requirements
- Python 3.8+
- 4GB RAM minimum
- 10GB disk space

### API Requirements
- **Twitter**: Developer account with API v2 access
- **Facebook**: App with Graph API permissions
- **YouTube**: Google Cloud project with YouTube Data API v3

## ğŸ› ï¸ Installation

### 1. Clone the Repository
```bash
git clone <repository-url>
cd social_media_analytics
```

### 2. Create Virtual Environment
```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

### 3. Install Dependencies
```bash
pip install -r requirements.txt
```

### 4. Configure Environment Variables
```bash
# Copy the example environment file
cp env_example.txt .env

# Edit .env with your API credentials
nano .env  # or use your preferred editor
```

### 5. Set Up API Credentials

#### Twitter API Setup
1. Go to [Twitter Developer Portal](https://developer.twitter.com/en/portal/dashboard)
2. Create a new app
3. Generate API keys and tokens
4. Add to your `.env` file

#### Facebook API Setup
1. Go to [Facebook Developers](https://developers.facebook.com/)
2. Create a new app
3. Generate access tokens
4. Add to your `.env` file

#### YouTube API Setup
1. Go to [Google Cloud Console](https://console.developers.google.com/)
2. Create a new project
3. Enable YouTube Data API v3
4. Generate API key
5. Add to your `.env` file

## ğŸš€ Usage

### Basic Pipeline Execution

```python
from pipeline import SocialMediaAnalyticsPipeline

# Initialize pipeline
pipeline = SocialMediaAnalyticsPipeline()

# Run with custom queries
results = pipeline.run_pipeline(
    queries=['#tech', '#AI', 'machine learning'],
    save_to_db=True,
    save_to_files=True
)

print(f"Pipeline completed: {results['success']}")
print(f"Posts collected: {results['posts_collected']}")
```

### Custom Data Collection

```python
# Collect from specific users
user_ids = {
    'twitter': '123456789',
    'facebook': '987654321',
    'youtube': 'UC123456789'
}

posts = pipeline.collect_data(
    user_ids=user_ids,
    limit_per_platform=50
)
```

### Analytics Generation

```python
from processor import DataProcessor

processor = DataProcessor()

# Generate analytics summary
summary = processor.generate_analytics_summary(posts)

# Get platform comparison
comparison = processor.get_platform_comparison(df)

# Detect anomalies
anomalies = processor.detect_anomalies(df, threshold=2.0)
```

### Airflow Automation

1. **Install Airflow**:
```bash
pip install apache-airflow
```

2. **Set Airflow home**:
```bash
export AIRFLOW_HOME=./airflow
airflow db init
```

3. **Copy DAG**:
```bash
cp dags/social_media_analytics_dag.py $AIRFLOW_HOME/dags/
```

4. **Start Airflow**:
```bash
airflow webserver --port 8080
airflow scheduler
```

## ğŸ“Š Data Models

### Post Model
```python
class Post(BaseModel):
    post_id: str
    platform: Platform
    content: str
    author_id: str
    author_name: str
    likes: int
    comments: int
    shares: int
    post_date: datetime
    engagement_score: int  # Computed field
```

### Analytics Summary
```python
class AnalyticsSummary(BaseModel):
    date: str
    daily_metrics: List[DailyMetrics]
    top_posts_overall: List[TopPost]
    top_posts_per_platform: Dict[str, List[TopPost]]
    moving_averages: List[MovingAverage]
```

## ğŸ”§ Configuration

### Environment Variables

| Variable | Description | Default |
|----------|-------------|---------|
| `DATABASE_URL` | Database connection string | `sqlite:///social_media_analytics.db` |
| `POSTS_LIMIT` | Maximum posts per platform | `1000` |
| `LOOKBACK_DAYS` | Days to look back for data | `7` |
| `OUTPUT_FORMAT` | Output file format | `json` |
| `LOG_LEVEL` | Logging level | `INFO` |

### Pipeline Settings

```python
# In config.py
BATCH_SIZE = 100          # Posts per batch
MAX_RETRIES = 3           # API retry attempts
RETRY_DELAY = 5           # Seconds between retries
```

## ğŸ“ Project Structure

```
social_media_analytics/
â”œâ”€â”€ collectors/                 # Data collection modules
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ base_collector.py      # Abstract base class
â”‚   â”œâ”€â”€ twitter_collector.py   # Twitter API collector
â”‚   â”œâ”€â”€ facebook_collector.py  # Facebook API collector
â”‚   â””â”€â”€ youtube_collector.py   # YouTube API collector
â”œâ”€â”€ dags/                      # Airflow DAGs
â”‚   â””â”€â”€ social_media_analytics_dag.py
â”œâ”€â”€ models.py                  # Data models and schemas
â”œâ”€â”€ processor.py               # Data processing and analytics
â”œâ”€â”€ storage.py                 # Data storage handlers
â”œâ”€â”€ pipeline.py                # Main pipeline orchestrator
â”œâ”€â”€ config.py                  # Configuration management
â”œâ”€â”€ requirements.txt           # Python dependencies
â”œâ”€â”€ env_example.txt           # Environment variables template
â””â”€â”€ README.md                 # This file
```

## ğŸ§ª Testing

### Run Tests
```bash
# Install test dependencies
pip install pytest pytest-asyncio

# Run tests
pytest tests/
```

### Test Coverage
```bash
pip install pytest-cov
pytest --cov=. tests/
```

## ğŸ“ˆ Monitoring and Logging

### Logging Configuration
- **Structured logging** with structlog
- **JSON format** for easy parsing
- **Multiple log levels** (DEBUG, INFO, WARNING, ERROR)
- **File and console output**

### Pipeline Monitoring
- **Success/failure tracking**
- **Execution time monitoring**
- **Data quality metrics**
- **Error rate tracking**

## ğŸ”’ Security Considerations

- **API keys** stored in environment variables
- **No hardcoded credentials** in source code
- **Rate limiting** to respect API quotas
- **Error logging** without sensitive data exposure

## ğŸš¨ Troubleshooting

### Common Issues

1. **Authentication Errors**
   - Verify API credentials in `.env` file
   - Check API permissions and quotas
   - Ensure tokens haven't expired

2. **Rate Limiting**
   - Reduce `POSTS_LIMIT` in configuration
   - Increase `RETRY_DELAY` between requests
   - Check API rate limit documentation

3. **Data Quality Issues**
   - Verify data models match API responses
   - Check for missing required fields
   - Review error logs for specific issues

### Debug Mode
```python
import logging
logging.basicConfig(level=logging.DEBUG)

# Run pipeline with debug logging
pipeline = SocialMediaAnalyticsPipeline()
```

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Add tests for new functionality
5. Submit a pull request

## ğŸ“„ License

This project is licensed under the MIT License - see the LICENSE file for details.

## ğŸ™ Acknowledgments

- **Twitter API** for social media data access
- **Facebook Graph API** for platform integration
- **YouTube Data API** for video analytics
- **Apache Airflow** for workflow automation
- **Pydantic** for data validation

## ğŸ“ Support

For questions and support:
- Create an issue in the repository
- Check the troubleshooting section
- Review the configuration documentation

---

**Note**: This pipeline requires valid API credentials from the respective social media platforms. Make sure to comply with each platform's terms of service and rate limiting policies.
